{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee7e33-f627-44d1-b0ce-942642a93a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_and_import(package_name):\n",
    "    try:\n",
    "        __import__(package_name)\n",
    "        print(f\"{package_name} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"{package_name} is not installed. Installing it now...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "            print(f\"{package_name} has been successfully installed.\")\n",
    "            __import__(package_name)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during installation of {package_name}: {e}\")\n",
    "\n",
    "# Check and install nltk and spacy\n",
    "install_and_import(\"nltk\")\n",
    "# install_and_import(\"spacy\")\n",
    "install_and_import(\"gensim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ffb09d-6c33-4199-8e2e-189e6d9ce7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ściągnięte corpusy (na początku zapewne nie będzie tego za wiele)\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown as cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116d2a35-4b80-4b10-92a0-dc5ac01d44cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cb.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab83704-d34e-4f19-8d95-87a197123b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\", \".join(cb.words()[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a732e-295a-42f0-9a1d-1cf1fe1e35ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb.words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6642f802-0d57-4cee-b1bd-f16844834234",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb.tagged_sents(categories='news')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702c6246-5a3e-4a1e-bc87-3307b8cde62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('inaugural')\n",
    "from nltk.corpus import inaugural\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(12, 6), dpi=80)\n",
    "cfd = nltk.ConditionalFreqDist((target, fileid[:4]) \n",
    "                               for fileid in inaugural.fileids() \n",
    "                               for w in inaugural.words(fileid) \n",
    "                               for target in ['america', 'citizen'] \n",
    "                               if w.lower().startswith(target))\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598222f1-65dc-4da7-aa06-d5771330b997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3756ede1-1522-44b2-95a4-d16feb4d0665",
   "metadata": {
    "id": "29a9c987"
   },
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da1eeb5-aeb2-48b9-aab7-c30a9e977e74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q8PyLz9VzQnY",
    "outputId": "c58be179-ec68-4d9a-8b4d-501ceb1bf406"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3641089b-2b1e-4ca7-aefa-1dec92764ae2",
   "metadata": {
    "id": "598e378e"
   },
   "outputs": [],
   "source": [
    "text1 = ('''\n",
    "The witcher halted at a distance of ten paces.  His sword, slowly drawn from its black enameled sheath, glistened and glowed above his head.\n",
    "“It’s silver,” he said.  “This blade is silver.”\n",
    "The pale little face did not flinch; the anthracite eyes did not change expression.\n",
    "“You’re so like a rusalka, “the witcher continued calmly, “that you could deceive anyone.  All the more as you’re a rare bird, black-haired one.  But horses are never mistaken.  They recognize creatures like you instinctively and perfectly.  What are you?  I think you’re a moola, or an alpor.  An ordinary vampire couldn’t come out in the sun.”\n",
    "The corners of the pale lips quivered and turned up a little.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ecffc0-871d-477d-8580-ba7278f921b1",
   "metadata": {
    "id": "d1faa5df"
   },
   "outputs": [],
   "source": [
    "text2 = ('''\n",
    "Born and raised in the Austrian Empire, Joe Tesla studied engineering and physics in the 1870s without receiving a \n",
    "degree, gaining practical experience in the early 1880s working in telephony and at Continental Edison in the \n",
    "new electric power industry. In 1884 he emigrated to the United States, where he became a naturalized citizen. \n",
    "He worked for a short time at the Edison Machine Works in New York City before he struck out on his own. \n",
    "With the help of partners to finance and market his ideas, Nicola Tesla set up laboratories and companies in \n",
    "New York to develop a range of electrical and mechanical devices. His alternating current (AC) induction \n",
    "motor and related polyphase AC patents, licensed by Westinghouse Electric in 1888, earned him a considerable \n",
    "amount of money and became the cornerstone of the polyphase system which that company eventually marketed.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa2684-11b9-43c9-8d5b-57990e63172e",
   "metadata": {
    "id": "b0766b53"
   },
   "outputs": [],
   "source": [
    "text3 = ('''\n",
    "The huge black eyes narrowed.\n",
    "“Where is he, black-haired one?  You were singing, so you’ve drunk some blood.  You’ve taken the ultimate measure, which means you haven’t managed to enslave his mind.  Am I right?”\n",
    "The black-tressed head nodded slightly, almost imperceptibility, and the corners of the mouth turned up even more.  The tiny little face took on an eerie expression.\n",
    "“No doubt you consider yourself the lady of this castle now?”\n",
    "A nod, this time clearer.\n",
    "“Are you a moola?”\n",
    "A slow shake of the head.  The hiss which reverberated through his bones could only have come from the pale, ghastly, smiling lips, although the witcher didn’t see them move.\n",
    "“Alpor?”\n",
    "Denial.\n",
    "The witcher backed away and clasped the hilt of his sword tighter.  “That means you’re-”\n",
    "The corners of the lips started to turn up higher and higher, the lips flew open…\n",
    "“A bruxa!” The witcher shouted, throwing himself towards the fountain.\n",
    "From behind the pale lips glistened white, spiky fangs.  The vampire jumped up, arched her back like a leopard and screamed.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2835401d-2e44-4698-a731-b5af964e98fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Function to clean the text by removing punctuation\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# Function to calculate term frequency (TF) for one document\n",
    "def tf(word, text):\n",
    "    cleaned_text = clean_text(text)\n",
    "    word_counts = Counter(cleaned_text.split())\n",
    "    # simple version\n",
    "    # total_words = sum(word_counts.values())\n",
    "    # tf_value = word_counts[word] / total_words if total_words else 0\n",
    "    l2_norm = np.sqrt(sum((count ** 2) for count in word_counts.values()))\n",
    "    tf_value = word_counts[word] / l2_norm if l2_norm else 0\n",
    "    return tf_value\n",
    "\n",
    "# Function to count how many documents contain the word\n",
    "def n_containing(word, texts):\n",
    "    return sum(1 for text in texts if word in clean_text(text).split())\n",
    "\n",
    "# Function to calculate inverse document frequency (IDF) for all documents\n",
    "def idf(word, texts):\n",
    "    n = n_containing(word, texts)\n",
    "    # simple version\n",
    "    # idf_value = math.log(len(texts) / (n if n else 1))\n",
    "    idf_value = math.log((len(texts) + 1) / (n + 1)) + 1\n",
    "    return idf_value\n",
    "\n",
    "# Function to calculate TF-IDF\n",
    "def tfidf(word, text, texts):\n",
    "    return tf(word, text) * idf(word, texts)\n",
    "\n",
    "# Function to print the TF-IDF table\n",
    "def print_tfidf_table(sample_words, texts):\n",
    "    print(f\"{'Word':<15}{'Text':<10}{'TF':<10}{'IDF':<10}{'TF-IDF':<10}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    # Loop through each word in the sample words\n",
    "    for word in sample_words:\n",
    "        # Print for each text\n",
    "        for idx, text in enumerate(texts):\n",
    "            tf_score = tf(word, text)\n",
    "            idf_score = idf(word, texts)\n",
    "            tfidf_score = tfidf(word, text, texts)\n",
    "            print(f\"{word:<15}Text {idx+1:<7}{tf_score:<10.4f}{idf_score:<10.4f}{tfidf_score:<10.4f}\")\n",
    "        \n",
    "        # After printing the word three times (once per text), add a blank row\n",
    "        print(\"\")  # Blank line for separation between words\n",
    "\n",
    "\n",
    "corpus = [text1.lower(), text2.lower(), text3.lower()]\n",
    "\n",
    "sample_words = ['sword', 'witcher', 'tesla', 'in', 'vampire', 'the']\n",
    "\n",
    "# Print the TF-IDF table\n",
    "print_tfidf_table(sample_words, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815374cc-3ae3-42c5-8acf-3b96c59e48f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# Use CountVectorizer to vectorize the text\n",
    "count_vect = CountVectorizer()\n",
    "text_counts = count_vect.fit_transform(corpus)\n",
    "\n",
    "# Get the vocabulary (mapping of words to index)\n",
    "vocabulary = count_vect.vocabulary_\n",
    "\n",
    "# Use TfidfTransformer with use_idf=False to get TF values\n",
    "tf_transformer = TfidfTransformer(use_idf=False)\n",
    "text_tf = tf_transformer.fit_transform(text_counts)\n",
    "\n",
    "# Use TfidfTransformer to compute TF-IDF values (default settings)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "text_tfidf = tfidf_transformer.fit_transform(text_counts)\n",
    "\n",
    "# Get the IDF values (Inverse Document Frequency)\n",
    "idf_values = tfidf_transformer.idf_\n",
    "def print_tfidf_table_sklearn(sample_words, count_vect, text_tf, idf_values, text_tfidf):\n",
    "    print(f\"{'Word':<15}{'Text':<10}{'TF':<10}{'IDF':<10}{'TF-IDF':<10}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    # Loop through each word in the sample words\n",
    "    for word in sample_words:\n",
    "        if word in vocabulary:\n",
    "            word_index = vocabulary[word]\n",
    "            # Extract TF values for this word across all texts\n",
    "            tf_values_for_word = text_tf[:, word_index].toarray().flatten()\n",
    "            # Extract TF-IDF values for this word across all texts\n",
    "            tfidf_values_for_word = text_tfidf[:, word_index].toarray().flatten()\n",
    "            # Get the IDF value for this word\n",
    "            idf_value = idf_values[word_index]\n",
    "            \n",
    "            # Print TF, IDF, and TF-IDF values for each text\n",
    "            for idx in range(len(tf_values_for_word)):\n",
    "                print(f\"{word:<15}Text {idx+1:<7}{tf_values_for_word[idx]:<10.4f}{idf_value:<10.4f}{tfidf_values_for_word[idx]:<10.4f}\")\n",
    "            print()  # Add an empty row after each word\n",
    "        else:\n",
    "            # If the word is not in the vocabulary, display 0 for all values\n",
    "            for idx in range(len(textlist)):\n",
    "                print(f\"{word:<15}Text {idx+1:<7}0.0000     0.0000     0.0000     \")\n",
    "            print()  # Add an empty row after each word\n",
    "\n",
    "\n",
    "sample_words = ['sword', 'witcher', 'tesla', 'in', 'vampire', 'the']\n",
    "print_tfidf_table_sklearn(sample_words, count_vect, text_tf, idf_values, text_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b629d54d-a6ac-43b3-b891-a1b0ea69e1ae",
   "metadata": {
    "id": "5504e91a"
   },
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028ba9fe-3c2d-401f-acfd-91e7e00160d5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hRfXDbEfyPz-",
    "outputId": "84dfc425-0759-4b1c-ac2f-1ea30419f074"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e4f015-79b2-403a-941c-8be9e8b92da2",
   "metadata": {
    "id": "582aaa16"
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from nltk.corpus import brown    \n",
    "sentences = brown.sents()\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#Proszę wytrenować model Word2Vec za pomocą korpusu brown, długość embeddingu - 100, wielkość okna - 5, \n",
    "#wziąć pod uwagę słowo jeśli występuje chociaż raz, liczbę epok ustawić na 10\n",
    "#na końcu zapisać model\n",
    "model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, epochs=10)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a43086c-30fe-4c82-90d9-4afe30bfb8c2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0d0642d",
    "outputId": "30359192-2596-46d2-f2d2-65690a986988"
   },
   "outputs": [],
   "source": [
    "#Jak wygląda embedding dla przykładowego słowa?\n",
    "model.wv['computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9daeb6-b2f7-4397-b184-39527d314728",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1tGIU6YoMI21",
    "outputId": "7fe71072-870d-4fbb-e533-389964950b60"
   },
   "outputs": [],
   "source": [
    "model.wv['computer'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45249f36-ee8e-4590-9c27-81eee2e6474d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "610fac55",
    "outputId": "0f3e68a7-d24d-4888-9133-f07e6c372bcd"
   },
   "outputs": [],
   "source": [
    "#Jakie jest 10 najbliższych wektorów do słówka 'wine'?\n",
    "model.wv.most_similar('wine', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb5b64c-602b-4e59-9cab-dbf6f9d1ba51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QwiUwgJqQt3",
    "outputId": "da7de458-d997-4715-f3c4-55493dc0a4f9"
   },
   "outputs": [],
   "source": [
    "nltk.download('webtext')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e1ab9-19bf-414f-9ff3-b7f74234c841",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fPG0aL06rQ6R",
    "outputId": "e9fe831b-d2a3-44c8-e3b3-db3b67441c1e"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a42ba1-70ef-45d4-b48d-3db69352a123",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eae89028",
    "outputId": "763e01f6-4828-435f-b1e7-f9798c2d7fb9"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext   \n",
    "sentences_web = webtext.sents()\n",
    "\n",
    "#Proszę załadować zapisany model i kontynuować trenowanie dla corpusu webtext, przez 4 epoki\n",
    "model2 = Word2Vec.load(\"word2vec.model\")\n",
    "model2.train(sentences_web, total_examples=1, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7086f8-287d-46df-a3d0-24682ea52e71",
   "metadata": {
    "id": "Wuqamj59GrLw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee59534-2ff7-4154-a06e-ac5b0a1d8d6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6aa95583",
    "outputId": "cacdc960-7c1d-4a8a-98f4-cab01285f56d"
   },
   "outputs": [],
   "source": [
    "#Jakie jest 10 najbliższych wektorów do słówka 'wine' teraz?\n",
    "model2.wv.most_similar('wine', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06503b60-33e5-41b3-89ab-88488960e331",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "8c7ee0a6",
    "outputId": "c1dba4e4-04f1-4911-aa0c-43c8ea6debfa"
   },
   "outputs": [],
   "source": [
    "#Co się stanie gdy się zapytamy o niewystępujące słowo?\n",
    "model2.wv.most_similar('witcher', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb93d2d-03d6-4dfc-bb62-3a83fe56264a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0af7246",
    "outputId": "d3235053-f4ac-439f-c5f4-2aae7a61062e"
   },
   "outputs": [],
   "source": [
    "#Proszę znaleść najbliższe wektory dla \"algebry emebddingów\": king - man + woman\n",
    "model2.wv.most_similar(model2.wv['king'] - model2.wv['man'] + model2.wv['woman'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad5af2b-771f-4069-beae-53edd14d1d89",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ac7a700",
    "outputId": "3d903487-1ff5-4466-94fb-69946a54b680"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "# Zobaczmy jak nazywają się wszystkie dostępne modele z embeddingami w gensim\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9224f6-cc9f-4467-8641-f3d460bdfd3e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "da53453a",
    "outputId": "1aa3c4f6-6708-4981-e67d-13b8bcc49dc8"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "w2v_vectors = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4342b5de-77b4-47e5-8323-aaeca2bf5072",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mMOUbM31r897",
    "outputId": "bc38eff8-e7ba-4776-9f92-604dbc8a5fcf"
   },
   "outputs": [],
   "source": [
    "w2v_vectors.most_similar(w2v_vectors['father'] - w2v_vectors['man'] + w2v_vectors['woman'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d51c12-460f-4dae-af6b-d342c1bbc68a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbqGGbSfsRd-",
    "outputId": "110c7f3e-4042-4bce-f817-3017e1c7178e"
   },
   "outputs": [],
   "source": [
    "w2v_vectors.most_similar('wine', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137c3b49-4975-4b56-8285-f93992871e7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1c65f851",
    "outputId": "fa0972d3-b944-4794-cd93-535d67a8f820"
   },
   "outputs": [],
   "source": [
    "#Jakie jest 10 najbliższych wektorów do słówka 'dog'?\n",
    "w2v_vectors.most_similar('dog', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21498557-5bb8-46c6-baeb-4e8466fb14fb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b3bd8fa5",
    "outputId": "721fda14-f837-4110-da3f-ebb6e51ab7d9"
   },
   "outputs": [],
   "source": [
    "#Proszę znaleść najbliższe wektory dla \"algebry emebddingów\": death - man + computer\n",
    "w2v_vectors.most_similar(w2v_vectors['death'] - w2v_vectors['man'] + w2v_vectors['computer'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ee7323-9094-4e9b-9f1e-e6cca6037dbb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eqt6zMnV13ph",
    "outputId": "e8588379-87bc-40cd-9e89-220800f7bc4b"
   },
   "outputs": [],
   "source": [
    "w2v_vectors['death'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3e5055-cd91-4f5b-a918-10c293a3a921",
   "metadata": {
    "id": "6f9ff8e6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition  import PCA\n",
    "import numpy as np\n",
    "\n",
    "def to_2d(embeddings):\n",
    "    # To reduce embedding dims without losing much information we use PCA\n",
    "    pca = PCA(n_components=2, whiten=True)\n",
    "    pca.fit(embeddings)\n",
    "    return pca.transform(embeddings)\n",
    "\n",
    "\n",
    "def annotated_scatter(points, names, color='blue'):\n",
    "    x_coords = points[:, 0]\n",
    "    y_coords = points[:, 1]\n",
    "    plt.scatter(x_coords, y_coords, c=color)\n",
    "    for label, x, y in zip(names, x_coords, y_coords):\n",
    "                      plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min() - .5, x_coords.max() + .5)\n",
    "    plt.ylim(y_coords.min() - .5, y_coords.max() + .5)\n",
    "\n",
    "    \n",
    "def plot_embeddings(embeddings, names, color='blue', show=True):\n",
    "    X_train = np.array([embeddings[k] for k in names])\n",
    "    embeddings_2d = to_2d(X_train)\n",
    "    \n",
    "    annotated_scatter(embeddings_2d, names, color)\n",
    "    plt.grid()\n",
    "    \n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb2fc52-b0a6-44e8-a2c6-d7e4554409ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "673140fb",
    "outputId": "9a8a25df-e8a4-4100-a4ae-580d7f4aa517"
   },
   "outputs": [],
   "source": [
    "near_dog = [elem[0] for elem in w2v_vectors.most_similar(w2v_vectors['dog'])]\n",
    "near_computer = [elem[0] for elem in w2v_vectors.most_similar(w2v_vectors['computer'])]\n",
    "len(near_computer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ff0bbd-06ba-4c33-9518-a50947091bc8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "9f5324ec",
    "outputId": "976c6bf5-245a-406a-d102-b1e0580c39eb"
   },
   "outputs": [],
   "source": [
    "plot_embeddings(w2v_vectors, \n",
    "                near_dog + near_computer, \n",
    "                color=['red'] * len(near_dog) + ['green'] * len(near_computer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b9d9b1-f15e-4f68-9dc5-390f2a7e8d4a",
   "metadata": {
    "id": "34cc9cff"
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib import cm\n",
    "\n",
    "LinearSubs = namedtuple('LinearSubs', \n",
    "                        ('word_pair', 'name'))\n",
    "\n",
    "def plot_linear_substructures(linear_subs, embeddings):\n",
    "    embeddings_matrix = [embeddings[p] for ls in linear_subs for p in ls.word_pair]\n",
    "    embeddings_matrix = np.array(embeddings_matrix)\n",
    "    pair_names = [p for ls in linear_subs for p in ls.word_pair]\n",
    "    ls_names = [ls.name for ls in linear_subs]\n",
    "    embeddings_2d = to_2d(embeddings_matrix)\n",
    "    annotated_scatter(embeddings_2d, \n",
    "                      pair_names, \n",
    "                      cm.Set1.colors[:len(embeddings_2d)])\n",
    "    \n",
    "    for i in range(0, len(embeddings_2d), 2):\n",
    "        p1 = embeddings_2d[i]\n",
    "        p2 = embeddings_2d[i + 1]\n",
    "        # Center of the linear substructure\n",
    "        center = [(p1[i] + p2[i]) / 2 + .04 for i in range(2)]\n",
    "        \n",
    "        plt.plot(*zip(p1, p2), '--')\n",
    "        plt.annotate(ls_names[i // 2], \n",
    "                     xy=center, \n",
    "                     xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b8f9ad-b3dd-40ca-b015-21d6fd520d9c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "aeec3fb5",
    "outputId": "40270782-45d4-4739-9ca0-bf6f1a3c036b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plot_linear_substructures([LinearSubs(('man', 'woman'), 'sex'),\n",
    "                           LinearSubs(('king', 'queen'), 'sex'),\n",
    "                           LinearSubs(('mother', 'father'), 'sex')], w2v_vectors)\n",
    "\n",
    "plt.subplot(132)\n",
    "plot_linear_substructures([LinearSubs(('cat', 'feline'), 'family'),\n",
    "                           LinearSubs(('dog', 'canine'), 'family'),\n",
    "                           LinearSubs(('parrot', 'bird'), 'family')], w2v_vectors)\n",
    "\n",
    "plt.subplot(133)\n",
    "plot_linear_substructures([LinearSubs(('samsung', 'mobile'), 'product'),\n",
    "                           LinearSubs(('sony', 'tv'), 'product'),\n",
    "                           LinearSubs(('ikea', 'furniture'), 'product')], w2v_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c94e486-e2bd-428c-b90e-01a4bb4170d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "\n",
    "# Define file paths\n",
    "zip_file = 'glove.6B.zip'\n",
    "extracted_folder = 'glove.6B'\n",
    "\n",
    "# Check if the zip file exists\n",
    "if not os.path.exists(zip_file):\n",
    "    print(f\"File not found! Downloading {zip_file}...\")\n",
    "    # Download the GloVe zip file\n",
    "    url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "    urllib.request.urlretrieve(url, zip_file)\n",
    "else:\n",
    "    print(f\"{zip_file} already exists. Skipping download.\")\n",
    "\n",
    "# Check if the folder with unzipped files exists\n",
    "if not os.path.exists(extracted_folder):\n",
    "    print(f\"Unzipping {zip_file}...\")\n",
    "    # Unzip the file\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_folder)\n",
    "else:\n",
    "    print(f\"{extracted_folder} already exists. Skipping unzip.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6645379-8de9-418e-9f76-052921dd1a37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdb0a4cf",
    "outputId": "40ce8804-32ed-43aa-8f24-4ddd7e69d373"
   },
   "outputs": [],
   "source": [
    "#Glove 6B\n",
    "\n",
    "# !curl -OL http://nlp.stanford.edu/data/glove.6B.zip -o glove.6B.zip\n",
    "#wget\n",
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512f37bb-c76c-4165-9f14-2d169e850840",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SKsZZYSAQ7Gt",
    "outputId": "9055a2db-30a6-4bab-f9f8-e71da7e44ac3"
   },
   "outputs": [],
   "source": [
    "# !unzip -o glove.6B.zip\n",
    "# !unzip -o /content/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e015ebf-2c29-4159-be40-761f6fe38f17",
   "metadata": {
    "id": "bef235f1"
   },
   "outputs": [],
   "source": [
    "glove_embeddings = {}\n",
    "with open('glove.6B/glove.6B.300d.txt') as f:\n",
    "    glove_embeddings = {l.split()[0]: np.array(l.split()[1:]).astype('float') for l in f}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735b07ad-0bfc-434d-85f2-8fb7268668be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fjxskutN4Bs-",
    "outputId": "1e8b2fce-af34-45ce-fa0f-93c44e477617"
   },
   "outputs": [],
   "source": [
    "glove_embeddings['computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a98a6d-0bd4-4db9-b26f-504f026b35cb",
   "metadata": {
    "id": "fbe78d30"
   },
   "outputs": [],
   "source": [
    "def get_closest(x, embeddings, topn=3):\n",
    "    \"\"\"\n",
    "    Get the closest embeddings calculating the euclidean distance\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: np.ndarray\n",
    "      Vector containing an embedding\n",
    "    top_k: int, optional\n",
    "      Get the top k similar embeddings\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "      Dict containing the top k similar embeddings to the given x\n",
    "    \"\"\"\n",
    "    # Stack all embeddings in a single matrix. Note: the matrix dimention will be\n",
    "    # V x D where V is the vocabulary size and D is the embedding dimension\n",
    "    embedding_matrix = np.array(list(embeddings.values()))\n",
    "    # Using broadcasting compute distance to each embedding in our vocabulary\n",
    "    distances = x - embedding_matrix\n",
    "    # Comoute the magnitude of each distance\n",
    "    distances = np.linalg.norm(distances, axis=1)\n",
    "    # Sort distance and keep the smallest k\n",
    "    min_idx = np.argsort(distances)[:topn]\n",
    "    return [list(embeddings)[i] for i in min_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059a31b1-46df-47ce-88da-79a5388ea50b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "9506e5fd",
    "outputId": "1b199d53-4053-439a-bead-0f5973981d4f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plot_linear_substructures([LinearSubs(('man', 'woman'), 'sex'),\n",
    "                           LinearSubs(('king', 'queen'), 'sex'),\n",
    "                           LinearSubs(('mother', 'father'), 'sex')], glove_embeddings)\n",
    "\n",
    "plt.subplot(132)\n",
    "plot_linear_substructures([LinearSubs(('cat', 'feline'), 'family'),\n",
    "                           LinearSubs(('dog', 'canine'), 'family'),\n",
    "                           LinearSubs(('parrot', 'bird'), 'family')], glove_embeddings)\n",
    "\n",
    "plt.subplot(133)\n",
    "plot_linear_substructures([LinearSubs(('samsung', 'mobile'), 'product'),\n",
    "                           LinearSubs(('sony', 'tv'), 'product'),\n",
    "                           LinearSubs(('ikea', 'furniture'), 'product')], glove_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35dbc59-25d7-4b7d-bddb-bb3186376831",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12414929",
    "outputId": "0c6929db-5eb0-4b66-ba9e-7e786f79035f"
   },
   "outputs": [],
   "source": [
    "get_closest(glove_embeddings['rome'] - glove_embeddings['italy'] + glove_embeddings['france'], glove_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82365027-a15f-4003-ac66-bd796f1ada24",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_closest(glove_embeddings['king'] - glove_embeddings['men'] + glove_embeddings['woman'], glove_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93e2a8d-852c-4ca6-bf82-eac8619b672e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
